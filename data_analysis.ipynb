{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('vae': conda)",
   "metadata": {
    "interpreter": {
     "hash": "eb5bc1cfb9acb33b1e6c4f7077eeee3518340c293d678b13d452bea5d718ad28"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/janek/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils.preprocess import preprocess_IMDB_sentence\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from convokit import Corpus, download, FightingWords, Utterance, Speaker\n",
    "from collections import Counter\n",
    "from torchtext.vocab import GloVe\n",
    "from utils.corpus import get_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_from_source(source, field, n):\n",
    "    df, _  = get_corpus(source, text_field=field, split_sentences=True, punct=False, to_ascii=True,\n",
    "                min_len=3, max_len=15,  test_size=0.0001, subsample_rows=n, save=False)\n",
    "    speaker = Speaker(meta={'name': source})\n",
    "    utts = [Utterance(id=f'{source}i', text=' '.join(t), speaker=speaker) for i, t in enumerate(df.utterance) ]\n",
    "    corp = Corpus(utterances=utts)\n",
    "    return df, corp\n",
    "\n",
    "def compare_corpora(c1, c2, n1, n2):\n",
    "    print(n1)\n",
    "    c1.print_summary_stats()\n",
    "    print(c1.get_utterances_dataframe().text.map(lambda x: x.split()).map(len).describe())\n",
    "    print('\\n'.join(c1.random_utterance().text for _ in range(10)))\n",
    "    print('\\n')\n",
    "    print(n2)\n",
    "    c2.print_summary_stats()\n",
    "    print(c2.get_utterances_dataframe().text.map(lambda x: x.split()).map(len).describe())\n",
    "    print('\\n'.join(c2.random_utterance().text for _ in range(10)))\n",
    "    ids1 = set(c1.get_utterance_ids())\n",
    "    corp = c1.merge(c2)\n",
    "    fw = FightingWords(ngram_range=(1,1))\n",
    "    fw.fit(corp, class1_func=lambda utt: utt.id in ids1, \n",
    "                 class2_func=lambda utt: utt.id not in ids1)\n",
    "    print(fw.summarize(corp, plot=False, class1_name=n1,\n",
    "                                          class2_name=n2))\n",
    "    return fw\n",
    "    \n",
    "def run_comparison(source1, field1, source2, field2, n=1000):\n",
    "    print('loading ', source1)\n",
    "    df1, c1 = corpus_from_source(source1, field1, n)\n",
    "    print('loading ', source2)\n",
    "    df2, c2 = corpus_from_source(source2, field2, n)\n",
    "    fw = compare_corpora(c1, c2, source1, source2)\n",
    "\n",
    "def word_freqs(data, voc, n=1000):\n",
    "    return Counter(w if w in voc else 'unk' for s in data.iloc[:n] for w in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imdb = pd.read_csv('data_options/IMDB Dataset.csv')\n",
    "# food = pd.read_csv('data_options/Reviews.csv')\n",
    "# movies = pd.read_csv('data_options/movies_metadata.csv')\n",
    "# tmbd = pd.read_csv('data_options/tmdb_5000_movies.csv')\n",
    "# hotels = pd.read_csv('data_options/tripadvisor_hotel_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data_options/google-10000-english-usa.txt', 'r') as fh:\n",
    "#     vocab1 = set(t.strip() for t in fh.readlines())\n",
    "# g = GloVe('6B',dim=50)\n",
    "# v = set(g.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading dataset...\n",
      "Dataset already exists at /Users/janek/.convokit/downloads/friends-corpus\n",
      "Cleaning\n",
      "108\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "df, c = corpus_from_source('friends-corpus', 'text', n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(c.random_utterance().text for _ in range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading  friends-corpus\n",
      "Downloading dataset...\n",
      "Dataset already exists at /Users/janek/.convokit/downloads/friends-corpus\n",
      "Cleaning\n",
      "108\n",
      "1\n",
      "loading  IMDB Dataset.csv\n",
      "Loading dataset from csv...\n",
      "Cleaning\n",
      "324\n",
      "1\n",
      "friends-corpus\n",
      "Number of Speakers: 1\n",
      "Number of Utterances: 1\n",
      "Number of Conversations: 1\n",
      "count    1.0\n",
      "mean     4.0\n",
      "std      NaN\n",
      "min      4.0\n",
      "25%      4.0\n",
      "50%      4.0\n",
      "75%      4.0\n",
      "max      4.0\n",
      "Name: text, dtype: float64\n",
      "okay it 's um\n",
      "okay it 's um\n",
      "okay it 's um\n",
      "okay it 's um\n",
      "okay it 's um\n",
      "okay it 's um\n",
      "okay it 's um\n",
      "okay it 's um\n",
      "okay it 's um\n",
      "okay it 's um\n",
      "\n",
      "\n",
      "IMDB Dataset.csv\n",
      "Number of Speakers: 1\n",
      "Number of Utterances: 1\n",
      "Number of Conversations: 1\n",
      "count     1.0\n",
      "mean     13.0\n",
      "std       NaN\n",
      "min      13.0\n",
      "25%      13.0\n",
      "50%      13.0\n",
      "75%      13.0\n",
      "max      13.0\n",
      "Name: text, dtype: float64\n",
      "i saw this stage show when it was broadcast on pbs in 1983\n",
      "i saw this stage show when it was broadcast on pbs in 1983\n",
      "i saw this stage show when it was broadcast on pbs in 1983\n",
      "i saw this stage show when it was broadcast on pbs in 1983\n",
      "i saw this stage show when it was broadcast on pbs in 1983\n",
      "i saw this stage show when it was broadcast on pbs in 1983\n",
      "i saw this stage show when it was broadcast on pbs in 1983\n",
      "i saw this stage show when it was broadcast on pbs in 1983\n",
      "i saw this stage show when it was broadcast on pbs in 1983\n",
      "i saw this stage show when it was broadcast on pbs in 1983\n",
      "\u001b[91mWARNING: \u001b[0mMultiple values found for Speaker(id: None, vectors: [], meta: {'name': 'IMDB Dataset.csv'}) for metadata key: 'name'. Taking the latest one found\n",
      "Initializing default CountVectorizer with ngram_range (1, 1)... Done.\n",
      "class1_func returned 1 valid corpus components. class2_func returned 1 valid corpus components.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "max_df corresponds to < documents than min_df",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-1fac33a505d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_comparison\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'friends-corpus'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'IMDB Dataset.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'review'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-fbc11c3a0545>\u001b[0m in \u001b[0;36mrun_comparison\u001b[0;34m(source1, field1, source2, field2, n)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loading '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mdf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_from_source\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mfw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompare_corpora\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mword_freqs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-fbc11c3a0545>\u001b[0m in \u001b[0;36mcompare_corpora\u001b[0;34m(c1, c2, n1, n2)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mfw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFightingWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     fw.fit(corp, class1_func=lambda utt: utt.id in ids1, \n\u001b[0;32m---> 23\u001b[0;31m                  class2_func=lambda utt: utt.id not in ids1)\n\u001b[0m\u001b[1;32m     24\u001b[0m     print(fw.summarize(corp, plot=False, class1_name=n1,\n\u001b[1;32m     25\u001b[0m                                           class2_name=n2))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/convokit/fighting_words/fightingWords.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, corpus, class1_func, class2_func, y, selector)\u001b[0m\n\u001b[1;32m    186\u001b[0m               \"class2_func returned {} valid corpus components.\".format(len(class1), len(class2)))\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngram_zscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bayes_compare_language\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ngram zscores computed.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/convokit/fighting_words/fightingWords.py\u001b[0m in \u001b[0;36m_bayes_compare_language\u001b[0;34m(self, class1, class2)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mclass2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclass2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mcounts_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mclass2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;31m# Now sum over languages...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1235\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmax_doc_count\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin_doc_count\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m                 raise ValueError(\n\u001b[0;32m-> 1237\u001b[0;31m                     \"max_df corresponds to < documents than min_df\")\n\u001b[0m\u001b[1;32m   1238\u001b[0m             X, self.stop_words_ = self._limit_features(X, vocabulary,\n\u001b[1;32m   1239\u001b[0m                                                        \u001b[0mmax_doc_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: max_df corresponds to < documents than min_df"
     ]
    }
   ],
   "source": [
    "run_comparison('friends-corpus', 'text', 'IMDB Dataset.csv', 'review', n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset already exists at /Users/janek/.convokit/downloads/friends-corpus\n"
     ]
    }
   ],
   "source": [
    "friends = Corpus(filename=download(\"friends-corpus\"))\n",
    "# supreme = Corpus(filename=download(\"supreme-corpus\"))\n",
    "# # parliament = Corpus(filename=download(\"parliament-corpus\"))\n",
    "# movie = Corpus(filename=download(\"movie-corpus\"))\n",
    "# diplomacy = Corpus(filename=download(\"diplomacy-corpus\")) # game, written\n",
    "# switchboard = Corpus(filename=download(\"switchboard-corpus\")) # conversational, from spoken\n",
    "# reddit = Corpus(filename=download('reddit-corpus-small'))\n",
    "# tennis = Corpus(filename=download('tennis-corpus'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset already exists at /Users/janek/.convokit/downloads/friends-corpus\n",
      "Dataset already exists at /Users/janek/.convokit/downloads/switchboard-corpus\n",
      "friends-corpus\n",
      "Number of Speakers: 700\n",
      "Number of Utterances: 67373\n",
      "Number of Conversations: 3107\n",
      "count    67373.000000\n",
      "mean         9.245454\n",
      "std         10.396307\n",
      "min          0.000000\n",
      "25%          2.000000\n",
      "50%          6.000000\n",
      "75%         13.000000\n",
      "max        196.000000\n",
      "Name: text, dtype: float64\n",
      "...now remember you have to imagine me in a kilt.\n",
      "No, no, no.\n",
      "Oh no, no, no, no, no, no. Don't get me wrong. No, he's not in like a sissy way. No, no, no... when he gets going, he can rattle a headboard like a sailor on leave...\n",
      "They won't be ready for weeks.\n",
      "I'm not supposed to tell you.\n",
      "I can't believe Ross went out with Rachel's sister! When Chandler made out with my sister I was mad at him for 10 years.\n",
      "Ya know, the man's got a point.\n",
      "Oh Ross!!\n",
      "This is a girl that I really like and had too swoop in there!\n",
      "Phoebe?\n",
      "\n",
      "\n",
      "switchboard-corpus\n",
      "Number of Speakers: 440\n",
      "Number of Utterances: 122646\n",
      "Number of Conversations: 1155\n",
      "count    122646.000000\n",
      "mean         16.737431\n",
      "std          22.078892\n",
      "min           1.000000\n",
      "25%           3.000000\n",
      "50%           8.000000\n",
      "75%          22.000000\n",
      "max         475.000000\n",
      "Name: text, dtype: float64\n",
      "{F Uh, } believe me, I do that myself. /\n",
      "-- that didn't do the trick  /{C and then } I got there and tried to, -/\n",
      "Yeah. /\n",
      "Yeah. /\n",
      "{C So } they're saying they're not as safe as, - /they give the appearance of being safe,  /{C but } they're really not as safe [ as they, + as they, ]  {F uh, } appear to be.  /{C So, }  {F uh, } I'm kind of concerned about that as  far as, {D you know, } [ w-, + ] the seat belts. /\n",
      "I was reading that in the supermarket line.  /I never have the nerve to buy the thing.  /{F Uh, } /\n",
      "years of life.  /# So much, #\n",
      "Oh. /\n",
      "I'm not sure I could do much better today.  /{C But, } {F uh, } that is also effecting buying a new car <laughter>. /\n",
      "Yeah,  /that's right.  /[ I, + I  ] take it you haven't done a whole lot of camping in this area. /\n",
      "\u001b[91mWARNING: \u001b[0mFound conflicting values for Corpus metadata key: 'name'. Overwriting with other Corpus's metadata.\n",
      "Initializing default CountVectorizer with ngram_range (1, 1)... Done.\n",
      "class1_func returned 67373 valid corpus components. class2_func returned 122646 valid corpus components.\n",
      "Vocab size is 5855\n",
      "Comparing language...\n",
      "ngram zscores computed.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          z-score          class\n",
       "ngram                           \n",
       "uh    -103.397367  supreme court\n",
       "and    -71.331281  supreme court\n",
       "they   -62.526009  supreme court\n",
       "huh    -61.455047  supreme court\n",
       "of     -50.703762  supreme court\n",
       "...           ...            ...\n",
       "this    57.471653        friends\n",
       "okay    59.311416        friends\n",
       "you     66.945929        friends\n",
       "me      67.194980        friends\n",
       "no      68.994542        friends\n",
       "\n",
       "[5855 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>z-score</th>\n      <th>class</th>\n    </tr>\n    <tr>\n      <th>ngram</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>uh</th>\n      <td>-103.397367</td>\n      <td>supreme court</td>\n    </tr>\n    <tr>\n      <th>and</th>\n      <td>-71.331281</td>\n      <td>supreme court</td>\n    </tr>\n    <tr>\n      <th>they</th>\n      <td>-62.526009</td>\n      <td>supreme court</td>\n    </tr>\n    <tr>\n      <th>huh</th>\n      <td>-61.455047</td>\n      <td>supreme court</td>\n    </tr>\n    <tr>\n      <th>of</th>\n      <td>-50.703762</td>\n      <td>supreme court</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>this</th>\n      <td>57.471653</td>\n      <td>friends</td>\n    </tr>\n    <tr>\n      <th>okay</th>\n      <td>59.311416</td>\n      <td>friends</td>\n    </tr>\n    <tr>\n      <th>you</th>\n      <td>66.945929</td>\n      <td>friends</td>\n    </tr>\n    <tr>\n      <th>me</th>\n      <td>67.194980</td>\n      <td>friends</td>\n    </tr>\n    <tr>\n      <th>no</th>\n      <td>68.994542</td>\n      <td>friends</td>\n    </tr>\n  </tbody>\n</table>\n<p>5855 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": []
  }
 ]
}